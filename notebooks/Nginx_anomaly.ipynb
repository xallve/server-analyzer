{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c5eda-1974-4e5a-b09b-5461e8659515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, to_timestamp, window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d641d1c-46cf-4d0d-bfc0-09b128e9e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"Nginx Log Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893a076-3c33-4565-8578-b43cda72df07",
   "metadata": {},
   "source": [
    "Nginx logs example\n",
    "127.0.0.1 - - [10/Jul/2024:22:14:15 +0000] \"GET /index.html HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b544b60-c288-44b6-83af-6674301d7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to log file\n",
    "log_file = \"data/access.log\"\n",
    "\n",
    "# Read file\n",
    "logs_df = spark.read.text(log_file)\n",
    "\n",
    "# Regex to parse logs\n",
    "log_pattern = r'(\\S+) (\\S+) (\\S+) \\[(\\S+ +\\S+)\\] \"(\\S+) (\\S+)\\s*(\\S*) (\\S*)\" (\\d{3}) (\\d+) \"(.*?)\" \"(.*?)\"'\n",
    "\n",
    "# determine needed values from regex\n",
    "logs_df = logs_df.select(\n",
    "    regexp_extract('value', log_pattern, 1).alias('ip'),\n",
    "    regexp_extract('value', log_pattern, 4).alias('timestamp'),\n",
    "    regexp_extract('value', log_pattern, 5).alias('method'),\n",
    "    regexp_extract('value', log_pattern, 6).alias('endpoint'),\n",
    "    regexp_extract('value', log_pattern, 9).alias('status'),\n",
    "    regexp_extract('value', log_pattern, 10).alias('content_size')\n",
    ")\n",
    "\n",
    "logs_df = logs_df.withColumn(\"content_size\", col(\"content_size\").cast(\"integer\"))\n",
    "logs_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660bd14-96d0-44f3-80bd-893cba64a772",
   "metadata": {},
   "source": [
    "Let's add the necessary signs to detect anomalies:\n",
    "\n",
    "The number of requests from the same IP address in different time windows.\n",
    "Average response size for each IP address in different time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775703bb-23bb-45e8-b668-5d3fe7426587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change string datetime to timestamp\n",
    "logs_df = logs_df.withColumn('timestamp', to_timestamp(logs_df.timestamp, 'dd/MMM/yyyy:HH:mm:ss Z'))\n",
    "\n",
    "# 5 min window for data agregation\n",
    "windowed_logs_df = logs_df.groupBy(window(\"timestamp\", \"5 minutes\"), \"ip\") \\\n",
    "    .agg(\n",
    "        count(\"ip\").alias(\"request_count\"),\n",
    "        avg(\"content_size\").alias(\"avg_content_size\")\n",
    "    )\n",
    "\n",
    "windowed_logs_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77632e3a-e9a7-43d1-8379-d7ed2c9166b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using VectorAssembler to prepare data\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"request_count\", \"avg_content_size\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "feature_vector = assembler.transform(windowed_logs_df)\n",
    "feature_vector.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caaccc-a45b-45f9-88f5-d3530e842aeb",
   "metadata": {},
   "source": [
    "Anomaly Detection Using KMeans\n",
    "Let's use the KMeans algorithm to detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7456d-f08b-49a3-aed3-8fa99fc45726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating KMeans model\n",
    "kmeans = KMeans(k=2, seed=1)  # Количество кластеров k можно настроить\n",
    "model = kmeans.fit(feature_vector.select(\"features\"))\n",
    "\n",
    "predictions = model.transform(feature_vector)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette with squared euclidean distance = {silhouette}\")\n",
    "\n",
    "# Detecting anomlies\n",
    "predictions.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f29ce2-e3df-4321-8f69-1c2fee0c266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to Pandas DataFrame\n",
    "predictions_pd = predictions.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(predictions_pd[\"request_count\"], predictions_pd[\"avg_content_size\"], c=predictions_pd[\"prediction\"], cmap=\"viridis\")\n",
    "plt.xlabel(\"Request Count\")\n",
    "plt.ylabel(\"Average Content Size\")\n",
    "plt.title(\"Nginx Log Clustering\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d44804-d9de-404f-8638-0be900469b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
